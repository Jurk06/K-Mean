{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-Means.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoLN6M85VvdM"
      },
      "source": [
        "# What is K-Mean?\n",
        "* K-Means Clustering is a simple yet powerful algorithm in data science\n",
        "* K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster.\n",
        "*  The main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loh9S11LakZT"
      },
      "source": [
        "# How K mean cluster works?\n",
        "* Step 1: Choose the number of clusters k\n",
        "   > The first step in k-means is to pick the number of clusters, k.\n",
        "* Step 2: Select k random points from the data as centroids\n",
        "   > Next, we randomly select the centroid for each cluster. Let’s say we want to have 2 clusters, so k is equal to 2 here. We then randomly select the centroid:\n",
        "*  Step 3: Assign all the points to the closest cluster centroid\n",
        "   > Once we have initialized the centroids, we assign each point to the closest cluster centroid:\n",
        "   > Here you can see that the points which are closer to the red point are assigned to the red cluster whereas the points which are closer to the green point are assigned to the green cluster.\n",
        "*  Step 4: Recompute the centroids of newly formed clusters\n",
        "   > Now, once we have assigned all of the points to either cluster, the next step is to compute the centroids of newly formed clusters:\n",
        "   > Here, the red and green crosses are the new centroids.\n",
        "\n",
        "* Step 5: Repeat steps 3 and 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8iGLI7qbfKR"
      },
      "source": [
        "# Difference between **K-NN** and **K-Mean** clustering?\n",
        "* KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points,\n",
        "* while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebtj7LmrcYVA"
      },
      "source": [
        "# Explain about Hirarchical Clustering?\n",
        "* Let’s say we have the below points and we want to cluster them into groups:\n",
        "* We can assign each of these points to a separate cluster:\n",
        "* Now, based on the similarity of these clusters, we can combine the most similar clusters together and repeat this process until only a single cluster is left:\n",
        "* We are essentially building a hierarchy of clusters. \n",
        "* That’s why this algorithm is called hierarchical clustering.\n",
        "# Types of Hierarchical Clustering\n",
        "* There are mainly two types of hierarchical clustering:\n",
        "\n",
        "1.  Agglomerative hierarchical clustering\n",
        "2.  Divisive Hierarchical clustering\n",
        "\n",
        "# Agglomerative Hierarchical Clustering\n",
        "* We assign each point to an individual cluster in this technique\n",
        "* Suppose there are 4 data points\n",
        "* We will assign each of these points to a cluster and hence will have 4 clusters in the beginning:\n",
        "* Then, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left:\n",
        "* We are merging (or adding) the clusters at each step, right? Hence, this type of clustering is also known as additive hierarchical clustering.\n",
        "\n",
        "# Divisive Hierarchical Clustering\n",
        "* Divisive hierarchical clustering works in the opposite way\n",
        "* Instead of starting with n clusters (in case of n observations), we start with a single cluster and assign all the points to that cluster.\n",
        "* So, it doesn’t matter if we have 10 or 1000 data points. All these points will belong to the same cluster at the beginning:\n",
        "* Now, at each iteration, we split the farthest point in the cluster and repeat this process until each cluster only contains a single point:\n",
        "* We are splitting (or dividing) the clusters at each step, hence the name divisive hierarchical clustering.\n",
        "* Agglomerative Clustering is widely used in the industry and that will be the focus in this article. Divisive hierarchical clustering will be a piece of cake once we have a handle on the agglomerative type.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykUpPhvGgLXj"
      },
      "source": [
        "# Limitation of Hirarchical Clustering\n",
        "* Computational Complexity in time and space\n",
        "* No objective function is directly minimized\n",
        "* Sensitive to noice and outlier\n",
        "* Breaking large cluster\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHdzF0Sugu7n"
      },
      "source": [
        "# Time and space complexity of Hirarchical Clustering\n",
        "* Space Complexity:- O(n^2)\n",
        "  > n is number of points\n",
        "* Time Complexity: O(n^3)\n",
        "  > n is step size\n",
        "  > n^2 is proximity matrix \n",
        "  > Complexity can be reduced to O(n^2log(n))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUAOdSJ6hYuN"
      },
      "source": [
        "# Explain about DBSCAN? \n",
        "* DBSCAN clustering is an underrated yet super useful clustering algorithm for unsupervised learning problems\n",
        "* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfXkNMcmVq-I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}